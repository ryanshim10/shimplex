# Shimplex - Personal AI Plex (Lite)
# ì™¸ë¶€ LLM ì—°ê²° ì „ìš© - Pinehill ì˜ì¡´ì„± ì œê±°
# ì‚¬ìš©ë²•: python app.py

from fastapi import FastAPI, Request, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import os
import httpx
from datetime import datetime
from typing import Optional, List, Dict
import asyncio

app = FastAPI(title="Shimplex Lite", version="1.1.0")
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_FILE = "config.json"

class Config:
    """ì„¤ì • ê´€ë¦¬"""
    def __init__(self):
        self.data = self.load()
    
    def load(self) -> dict:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return self.default_config()
    
    def save(self):
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)
    
    def default_config(self) -> dict:
        return {
            "llm": {
                "provider": "openai",
                "api_key": "",
                "base_url": "",
                "model": "gpt-4o-mini",
                "temperature": 0.7
            },
            "app": {
                "host": "0.0.0.0",
                "port": 8080,
                "language": "ko"
            }
        }
    
    def get(self, key: str, default=None):
        keys = key.split('.')
        value = self.data
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        return value
    
    def set(self, key: str, value):
        keys = key.split('.')
        target = self.data
        for k in keys[:-1]:
            if k not in target:
                target[k] = {}
            target = target[k]
        target[keys[-1]] = value
        self.save()

config = Config()

# LLM í´ë¼ì´ì–¸íŠ¸
class LLMClient:
    def __init__(self):
        self.provider = config.get('llm.provider', 'openai')
        self.api_key = config.get('llm.api_key', '')
        self.base_url = config.get('llm.base_url', '')
        self.model = config.get('llm.model', 'gpt-4o-mini')
        self.temperature = config.get('llm.temperature', 0.7)
    
    async def chat(self, message: str, history: List[Dict] = None) -> str:
        """LLMê³¼ ëŒ€í™”"""
        if not self.api_key and self.provider != 'ollama':
            return "âŒ LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        try:
            if self.provider == 'openai':
                return await self._chat_openai(message, history)
            elif self.provider == 'anthropic':
                return await self._chat_anthropic(message, history)
            elif self.provider == 'ollama':
                return await self._chat_ollama(message, history)
            else:
                return await self._chat_custom(message, history)
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"
    
    async def _chat_openai(self, message: str, history: List[Dict] = None) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = [{"role": "system", "content": self._get_system_prompt()}]
        
        if history:
            for h in history[-10:]:  # ìµœê·¼ 10ê°œë§Œ
                messages.append({"role": h.get("role", "user"), "content": h.get("content", "")})
        
        messages.append({"role": "user", "content": message})
        
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60.0
            )
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
    
    async def _chat_anthropic(self, message: str, history: List[Dict] = None) -> str:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        
        system_prompt = self._get_system_prompt()
        
        # Anthropicì€ historyë¥¼ messagesì— í¬í•¨
        messages = []
        if history:
            for h in history[-10:]:
                role = "assistant" if h.get("role") == "ai" else "user"
                messages.append({"role": role, "content": h.get("content", "")})
        
        messages.append({"role": "user", "content": message})
        
        data = {
            "model": self.model or "claude-3-haiku-20240307",
            "max_tokens": 4096,
            "system": system_prompt,
            "messages": messages,
            "temperature": self.temperature
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=data,
                timeout=60.0
            )
            response.raise_for_status()
            result = response.json()
            return result['content'][0]['text']
    
    async def _chat_ollama(self, message: str, history: List[Dict] = None) -> str:
        base_url = self.base_url or "http://localhost:11434"
        
        messages = [{"role": "system", "content": self._get_system_prompt()}]
        
        if history:
            for h in history[-10:]:
                role = "assistant" if h.get("role") == "ai" else "user"
                messages.append({"role": role, "content": h.get("content", "")})
        
        messages.append({"role": "user", "content": message})
        
        data = {
            "model": self.model or "llama3.1:8b",
            "messages": messages,
            "stream": False
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{base_url}/api/chat",
                json=data,
                timeout=120.0
            )
            response.raise_for_status()
            result = response.json()
            return result['message']['content']
    
    async def _chat_custom(self, message: str, history: List[Dict] = None) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = [{"role": "system", "content": self._get_system_prompt()}]
        
        if history:
            for h in history[-10:]:
                messages.append({"role": h.get("role", "user"), "content": h.get("content", "")})
        
        messages.append({"role": "user", "content": message})
        
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60.0
            )
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
    
    def _get_system_prompt(self) -> str:
        return """ë‹¹ì‹ ì€ Shimplex AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‘ë‹µì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""

llm_client = LLMClient()

# ë©”ëª¨ë¦¬ ê¸°ë°˜ ëŒ€í™” ì €ì¥ (ì„¸ì…˜ë³„)
chat_histories = {}

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/config")
async def get_config():
    """ì„¤ì • ì¡°íšŒ (API í‚¤ëŠ” ì œì™¸)"""
    safe_config = {
        "llm": {
            "provider": config.get('llm.provider'),
            "model": config.get('llm.model'),
            "base_url": config.get('llm.base_url')
        },
        "app": config.get('app')
    }
    return safe_config

@app.post("/api/config")
async def update_config(data: dict):
    """ì„¤ì • ì—…ë°ì´íŠ¸"""
    if 'llm' in data:
        for key, value in data['llm'].items():
            config.set(f'llm.{key}', value)
    
    # ì„¤ì • ë³€ê²½ í›„ í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™”
    global llm_client
    llm_client = LLMClient()
    
    return {"status": "ok"}

class ChatMessage(BaseModel):
    message: str
    session_id: str = "default"

@app.post("/api/chat")
async def api_chat(chat: ChatMessage):
    """AI ì±„íŒ… API"""
    session_id = chat.session_id or "default"
    
    # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if session_id not in chat_histories:
        chat_histories[session_id] = []
    
    history = chat_histories[session_id]
    
    response = await llm_client.chat(chat.message, history)
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    history.append({"role": "user", "content": chat.message})
    history.append({"role": "ai", "content": response})
    
    # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
    if len(history) > 50:
        chat_histories[session_id] = history[-50:]
    
    return {
        "message": chat.message,
        "response": response,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/history/{session_id}")
async def get_history(session_id: str = "default"):
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    return chat_histories.get(session_id, [])

@app.delete("/api/history/{session_id}")
async def clear_history(session_id: str = "default"):
    """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
    if session_id in chat_histories:
        chat_histories[session_id] = []
    return {"status": "ok"}

@app.get("/api/health")
async def health_check():
    """ìƒíƒœ í™•ì¸"""
    return {
        "status": "ok",
        "llm_provider": config.get('llm.provider'),
        "llm_configured": bool(config.get('llm.api_key')),
        "version": "1.1.0"
    }

if __name__ == "__main__":
    import uvicorn
    
    # ì„¤ì • í™•ì¸
    if not os.path.exists(CONFIG_FILE):
        config.save()
        print(f"âœ… ê¸°ë³¸ ì„¤ì • ìƒì„±: {CONFIG_FILE}")
    
    host = config.get('app.host', '0.0.0.0')
    port = config.get('app.port', 8080)
    
    print(f"""
ğŸš€ Shimplex Lite ì‹œì‘!
ğŸ”— http://{host}:{port}

âš™ï¸ ì„¤ì • íŒŒì¼: {CONFIG_FILE}

ğŸ’¡ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”?
   1. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:{port} ì ‘ì†
   2. ì„¤ì • íƒ­ì—ì„œ LLM API í‚¤ ì…ë ¥
   3. ì±„íŒ… ì‹œì‘!

ğŸ“ ì§€ì› LLM:
   - OpenAI (GPT-4, GPT-4o-mini ë“±)
   - Anthropic (Claude 3 ë“±)
   - Ollama (ë¡œì»¬ AI)
   - Custom (OpenAI í˜¸í™˜ API)
""")
    
    uvicorn.run(app, host=host, port=port)
